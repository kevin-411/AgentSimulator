{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T15:04:23.020158Z",
     "start_time": "2025-06-02T15:04:15.536565Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import pm4py\n",
    "from pm4py.objects.log.util import dataframe_utils\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "from pm4py.visualization.petri_net import visualizer as pn_visualizer \n",
    "\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T15:04:23.166209Z",
     "start_time": "2025-06-02T15:04:23.150265Z"
    }
   },
   "source": [
    "def load_training_data():\n",
    "    # Get the base simulated_data directory\n",
    "    base_dir = \"../simulated_data\"\n",
    "    training_dfs = []\n",
    "    \n",
    "    # Check if directory exists\n",
    "    if not os.path.exists(base_dir):\n",
    "        print(f\"Directory {base_dir} does not exist\")\n",
    "        return training_dfs\n",
    "        \n",
    "    # Iterate through each folder in simulated_data\n",
    "    for folder in os.listdir(base_dir):\n",
    "        folder_path = os.path.join(base_dir, folder)\n",
    "        \n",
    "        # Check if it's a directory and contains main_results\n",
    "        main_results_path = os.path.join(folder_path, \"autonomous\")\n",
    "        if os.path.isdir(folder_path) and os.path.exists(main_results_path):\n",
    "            \n",
    "            # Look for train_preprocessed.csv\n",
    "            train_file = os.path.join(main_results_path, \"train_preprocessed.csv\")\n",
    "            if os.path.exists(train_file):\n",
    "                try:\n",
    "                    # Read the CSV file\n",
    "                    df = pd.read_csv(train_file)\n",
    "                    \n",
    "                    # Store the dataframe with folder name for reference\n",
    "                    training_dfs.append({\n",
    "                        \"folder\": folder,\n",
    "                        \"data\": df\n",
    "                    })\n",
    "                    print(f\"Loaded training data from {folder}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {train_file}: {str(e)}\")\n",
    "    \n",
    "    return training_dfs"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T15:04:24.832515Z",
     "start_time": "2025-06-02T15:04:23.175257Z"
    }
   },
   "source": [
    "dfs= load_training_data()"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T15:04:28.889425Z",
     "start_time": "2025-06-02T15:04:28.172700Z"
    }
   },
   "source": [
    "def rename_dataframe_columns(training_dfs):\n",
    "    # Column mapping dictionary\n",
    "    column_map = {\n",
    "        'end_timestamp': 'time:timestamp',\n",
    "        'case_id': 'case:concept:name',\n",
    "        'activity_name': 'concept:name',  # Note: using activity_name based on your codebase\n",
    "        'activity': 'concept:name'  # Including both variations to be safe\n",
    "    }\n",
    "    \n",
    "    logs = []\n",
    "    # Iterate through each dataset\n",
    "    for dataset in training_dfs:\n",
    "        # Rename columns that exist in the DataFrame\n",
    "        existing_columns = {old: new for old, new in column_map.items() \n",
    "                          if old in dataset['data'].columns}\n",
    "        \n",
    "        # Apply the renaming\n",
    "        dataset['data'].rename(columns=existing_columns, inplace=True)\n",
    "        \n",
    "        print(f\"Renamed columns in {dataset['folder']}\")\n",
    "        print(f\"New columns: {dataset['data'].columns.tolist()}\")\n",
    "\n",
    "        dataset['data']['start_timestamp'] = pd.to_datetime(dataset['data']['start_timestamp'], format='mixed')\n",
    "        dataset['data']['time:timestamp'] = pd.to_datetime(dataset['data']['time:timestamp'], format='mixed')\n",
    "\n",
    "        # dataset['data'] = dataframe_utils.convert_timestamp_columns_in_df(dataset['data'])\n",
    "        log = log_converter.apply(dataset['data'])\n",
    "        logs.append(log)\n",
    "    return logs\n",
    "logs= rename_dataframe_columns(dfs)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T15:04:30.475911Z",
     "start_time": "2025-06-02T15:04:28.982047Z"
    }
   },
   "source": [
    "for log in logs:\n",
    "    # generate full petri net\n",
    "    net, initial_marking, final_marking = pm4py.discover_petri_net_inductive(\n",
    "                                            log=log, \n",
    "                                            noise_threshold=0.0,\n",
    "                                            timestamp_key='start_timestamp',)\n",
    "    gviz = pn_visualizer.apply(net, initial_marking, final_marking)\n",
    "    pn_visualizer.view(gviz)"
   ],
   "outputs": [],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent_simulator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
